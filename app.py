import pydantic.v1
import streamlit as st
from langchain_groq import ChatGroq
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
import os
import pickle
import re;
from dotenv import load_dotenv
import fitz
from HTMLTEMPLATE import css
from langchain.schema import Document
# import pytesseract
# from PIL import Image




# pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # ุบููุฑ ุงููุณุงุฑ ุญุณุจ ููุงู ุงูุชุซุจูุช ุนูุฏู


def prepare_ocr_for_lm(text):
    """
    ุชูุธูู ุงููุต ุงููุงุชุฌ ูู OCR ูุจู ุฅุฑุณุงูู ูููููุฐุฌ
    """
    text = re.sub(r"\s{2,}", " ", text)
    text = re.sub(r"[^\u0600-\u06FF0-9A-Za-z\s.,()\-โโ/]", "", text)
    return text.strip()




def GET_TEXT_FROM_PDF(PDFS):
    """
    ๐ ุงุณุชุฎุฑุงุฌ ุงููุต ูู ูููุงุช PDF
    """
    full_text = ""

    for pdf in PDFS:
        pdf.seek(0)
        pdf_hash = hash(pdf.name)
        cache_file = f"cache_{pdf_hash}_smartocr.pkl"

        if os.path.exists(cache_file):
            with open(cache_file, "rb") as f:
                cached_text = pickle.load(f)
            st.info(f"๐ฆ ุชู ุชุญููู ุงููุต ูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ (Smart OCR Cache): {pdf.name}")
            full_text += cached_text
            continue

        pdf_text = ""

        doc = fitz.open(stream=pdf.read(), filetype="pdf")
        for i, page in enumerate(doc, start=1):
            text = page.get_text("text")
            if len(text.strip()) > 0:
                st.info(f"๐ ุงูุตูุญุฉ {i}: ุชุญุชูู ุนูู ูุต ูุงุจู ูููุณุฎ โ")
                pdf_text += text + "\n"
                continue

            st.info(f"โ ุงูุตูุญุฉ {i}: ูุง ุชุญุชูู ุนูู ูุตุ ุณูุชู ุงุณุชุฎุฏุงู Tesseract OCR...")

            # pix = page.get_pixmap(dpi=200)
            # img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

            # try:
            #     extracted = pytesseract.image_to_string(img, lang="ara+eng")
            # except Exception as e:
            #     st.error(f"โ ูุดู Tesseract OCR: {e}")
            #     extracted = ""

            # if extracted.strip():
            #     pdf_text += extracted + "\n"
            #     st.success(f"โ ุชู ุชุญููู ุงูุตูุญุฉ {i} ูุงุณุชุฎุฑุงุฌ ุงููุต ({len(extracted)} ุญุฑู).")
            # else:
            #     st.warning(f"โ ุงูุตูุญุฉ {i}: ูู ูุชููู OCR ูู ุงุณุชุฎุฑุงุฌ ุฃู ูุต.")

        pdf_text = re.sub(r"\s{2,}", " ", pdf_text).strip()

        with open(cache_file, "wb") as f:
            pickle.dump(pdf_text, f)

        full_text += pdf_text + "\n"

    st.success(f"๐ ุชู ุงุณุชุฎุฑุงุฌ ุงููุต ุจุงููุงูู ({len(full_text)} ุญุฑู).")

    cleaned_text = prepare_ocr_for_lm(full_text)
    return cleaned_text




def SPLITTEXTTOCHUNK(TEXT):
  SPLITTER = RecursiveCharacterTextSplitter(
    chunk_size=3500,
    chunk_overlap=500,
    separators=["\n", "ุงููุงุฏุฉ", ".", "ุ", " "]
  )
  CHUNK = SPLITTER.split_text(TEXT)
  return CHUNK


def CREATESTORE(TEXT, filename):
    """
    ๐ง ุฅูุดุงุก ุฃู ุชุญููู ูุงุนุฏุฉ ุจูุงูุงุช FAISS ุฎุงุตุฉ ุจูู ููู PDF.
    - ูุชู ุญูุธ ูู ููู ุฏุงุฎู ูุฌูุฏ faiss_index ุจุงุณู ุงูููู.
    """
    # ูุณุงุฑ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจุงูููู
    os.makedirs("faiss_index", exist_ok=True)
    PATH = os.path.join("faiss_index", f"{filename}")

    # ุงุฎุชูุงุฑ ูููุฐุฌ Embedding
    EMBEDDING = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    # ุงูุชุฃูุฏ ูู ูุฌูุฏ ููู ุงูููุฑุณ ูุจู ุงูุชุญููู
    if os.path.exists(f"{PATH}.index.faiss") and os.path.exists(f"{PATH}.index.faiss"):
        STORE = FAISS.load_local(PATH, EMBEDDING, index_name=filename ,allow_dangerous_deserialization=True)
        st.info(f"๐ฆ ุชู ุชุญููู ูุงุนุฏุฉ ุจูุงูุงุช {filename}")
    else:
        st.info(f"โ ุฌุงุฑู ุฅูุดุงุก ูุงุนุฏุฉ ุจูุงูุงุช ุฌุฏูุฏุฉ ููููู: {filename} ...")
        STORE = FAISS.from_texts(TEXT, embedding=EMBEDDING)
        STORE.save_local("faiss_index", index_name=filename)
        st.success(f"๐พ ุชู ุญูุธ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจุงูููู {filename} ุจูุฌุงุญ.")

    return STORE




def ASK_PDF_QUESTION(STORE, user_question):
    """
    ๐น ุงูุฏุงูุฉ ุฏู ุจุชุณุชูุจู ุณุคุงู ุงููุณุชุฎุฏูุ ูุชุณุชุฎุฏู ูุงุนุฏุฉ ุงูุจูุงูุงุช (FAISS)
    ุนูุดุงู ุชุฏูุฑ ุนูู ุฃูุณุจ ุฌุฒุก ูู ุงููุต ูุชุฌูุจ ุฅุฌุงุจุฉ ุฐููุฉ ูู ููุฏูู Groq.
    """

    # ูุจุญุซ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช ุนู ุงููุตูุต ุงูุฃูุฑุจ ููุณุคุงู
    docs = STORE.similarity_search(user_question, k=2)

    recent_context_text = ""
    if "chat_history" in st.session_state and len(st.session_state.chat_history) > 0:
        # ุฎุฐ ุขุฎุฑ 2 ูุญุงุฏุซุงุช (user+assistant)
        for chat in st.session_state.chat_history[-2:]:
            q = chat.get("question", "")
            a = chat.get("answer", "")
            recent_context_text += f"ูุญุงุฏุซุฉ ุณุงุจูุฉ โ ุณุคุงู ุงููุณุชุฎุฏู: {q}\nุฑุฏ ุงููุณุงุนุฏ: {a}\n\n"

    if recent_context_text:
        docs.append(Document(page_content=recent_context_text, metadata={"source": "recent_conversation"}))


    # ูุญุฏุฏ ุดูู ุงูุจุฑููุจุช (ุทุฑููุฉ ููู ุงูุณุคุงู)

    PROMPT = """
ุฃูุช ูุณุงุนุฏ ุฐูู ููุชุฎุตุต ูู ุชุญููู ุงููุตูุต ุจูุฎุชูู ุฃููุงุนูุง (ูุงููููุฉุ ูุงููุฉุ ุฅุฏุงุฑูุฉุ ุฃูุงุฏูููุฉุ ุฃู ุดุฎุตูุฉ). 
ูููุชู ููู ุงููุต ูุชุญูููู ุจุนูู ูุงุณุชุฎูุงุต ุงููุนูููุงุช ูุงูุฅุฌุงุจุงุช ุงููุทููุจุฉ ุจุฏูุฉ ูุชูุธูู.

โ๏ธ ููุงุนุฏ ุฃุณุงุณูุฉ:
- ุงููุชุบูุฑ {context} ูุญุชูู ุนูู ูุต ุงูููู ุฃู ุงููุณุชูุฏ ุงูุฐู ุฑูุนู ุงููุณุชุฎุฏู.
- ุฅุฐุง ูุงู {context} ูุญุชูู ุนูู ูุต ูุนูู โ ุงุจุฏุฃ ุงูุชุญููู ููุฑูุง.
- ุฅุฐุง ูุงู ูุงุฑุบูุง ุฃู ุบูุฑ ุตุงูุญ โ ุฃุฎุจุฑ ุงููุณุชุฎุฏู ููุท ุญูููุง ุฃูู ูู ููุฑุณู ุงูููู.
- ูุง ุชุณุชุฎุฏู ุฃู ูุนูููุฉ ุฎุงุฑุฌ ุงููุต ุงูููุฌูุฏ ูู {context}.

๐ฏ ุฃุณููุจ ุงูุชูุงุนู:
- ุงุณุชุฎุฏู ุฃุณููุจ ูุฏูู ุงุญุชุฑุงูู ุนูุฏ ุงูุฑุฏ ุนูู ุงููุณุชุฎุฏู (ุนุงููุฉ ุฃู ูุตุญู ุญุณุจ ุณูุงูู).
- ุนูุฏ ุงูุชุญููู ุงุณุชุฎุฏู ูุบุฉ ุนุฑุจูุฉ ูุตุญู ุฏูููุฉ ููุงุถุญุฉ.
- ุงุณุชุฎุฏู ุงูุฅูููุฌู ููุชูุธูู ุฏูู ูุจุงูุบุฉ (โ๏ธุ ๐ฐุ ๐ุ ๐งุ ๐ก...).
- ุงุฌุนู ุงูุฅุฌุงุจุฉ ููุธูุฉ ุจุนูุงููู ูุงุถุญุฉ ููุฃูุณุงู ุงููุฎุชููุฉ.

๐ ุงููุจุงุฏุฆ ุงูุนุงูุฉ:
1๏ธโฃ ุงุณุชูุชุฌ ุฃูููุง ุทุจูุนุฉ ุงููุณุชูุฏ ูู ุงููุต (ูุซูุงู: ูุงููููุ ูุงููุ ุฅุฏุงุฑูุ ุฃูุงุฏูููุ ุดุฎุตู...).
   - ุฃูุซูุฉ:
     - ุฅุฐุง ููุฌุฏุช ูููุงุช ูุซู "ูุงูููุ ูุงุฏุฉุ ูุฑุงุฑ" โ ูุงูููู.
     - ุฅุฐุง ููุฌุฏุช ุฃุฑูุงู ูุงููุฉ ุฃู ูุณุจ ุฃู ุถุฑุงุฆุจ โ ูุงูู.
     - ุฅุฐุง ููุฌุฏุช ุนุจุงุฑุงุช ูุซู "ุฅุฏุงุฑุฉุ ููุธูุ ูุฑุงุฑ ุฅุฏุงุฑู" โ ุฅุฏุงุฑู.
     - ุฅุฐุง ููุฌุฏุช ุนุจุงุฑุงุช ูุซู "ุฅุนุฏุงุฏ ุงูุทุงูุจุ ูุดุฑูุน ุชุฎุฑุฌ" โ ุฃูุงุฏููู.
     - ุฅุฐุง ููุฌุฏุช ุจูุงูุงุช ุดุฎุตูุฉ ุฃู ุณูุฑุฉ ุฐุงุชูุฉ โ ุดุฎุตู.
2๏ธโฃ ูุฏูู ุงูุชุญููู ุจูุงุกู ุนูู ููุน ุงููุณุชูุฏ ุงูููุชุดู.
3๏ธโฃ ูุถูุญ ุฏุงุฆููุง ุฅู ูุงูุช ุงููุนูููุฉ ูู ูุต ุตุฑูุญ ุฃู ูู ุงุณุชูุชุงุฌ (ูู: "ููููู ูู ุงูุณูุงู ุฃู...").
4๏ธโฃ ูุง ุชุฎุชูู ูุนูููุงุช ุบูุฑ ููุฌูุฏุฉ.

๐๏ธ ุชุญููู ุงููุคูู ุฃู ุงูุฌูุฉ:
- ุงุจุญุซ ุนู ุฃู ุนุจุงุฑุงุช ูุซู:
  "ุชุฃููู"ุ "ุฅุนุฏุงุฏ"ุ "ุจููู"ุ "ุชุญุช ุฅุดุฑุงู"ุ "ุฅุดุฑุงู"ุ "ูู ุฅุนุฏุงุฏ"ุ "ุฅุนุฏุงุฏ ูุชุญุฑูุฑ"ุ "ุฅุดุฑุงู ุงูุฃุณุชุงุฐ"ุ "ุฅุดุฑุงู ุฏ.".
- ุฅุฐุง ููุฌุฏ ุฃูุซุฑ ูู ุงุณูุ ูุฑูู ุจูููู (ูุคูู / ูุดุฑู / ูุนุฏู).
- ุฅุฐุง ูู ููุฐูุฑ ุงุณู ุตุฑูุญุ ุงุณุชูุชุฌ ุงูุฌูุฉ ูู ุงูุณูุงู (ูุซู ุงุณู ุฌุงูุนุฉ ุฃู ูุฒุงุฑุฉ ุฃู ูุคุณุณุฉ).
- ูุง ุชุถุน ุฃุณูุงุก ุบูุฑ ูุฐููุฑุฉ ุฃู ุจูุง ุฏููู.

๐ ุทุฑููุฉ ุงูุนุฑุถ:
ุงุจุฏุฃ ุฏุงุฆููุง ุจู:
๐ **ููุน ุงููุณุชูุฏ:** (ุงุณุชูุชุงุฌู ูู ุงููุต)

ุซู ุงุนุฑุถ ุงูุชุญููู ุจุงูุชูุธูู ุงูุชุงูู ุนูุฏ ุงูุญุงุฌุฉ:
- โ๏ธ **ุงูุชุญููู ุงููุงูููู**
- ๐ฐ **ุงูุชุญููู ุงููุงูู**
- ๐ **ุงูุชุญููู ุงูุฅุฏุงุฑู**
- ๐ **ุงูุชุญููู ุงูุฃูุงุฏููู**
- ๐ค **ุงูุชุญููู ุงูุดุฎุตู**
- ๐๏ธ **ุงููุคูู ุฃู ุงูุฌูุฉ ุงููุนุฏูุฉ**
- ๐ **ุงูุฎูุงุตุฉ:** (ุณุทุฑูู ูุฃูู ุงููุชุงุฆุฌ)

๐ก ูู ุญุงู ูุฌูุฏ ุญุณุงุจุงุช ูุงููุฉ ุฃู ูุณุจ:
ุงูุชุจูุง ุจุฎุทูุงุช ูุงุถุญุฉ:
- ุงููููุฉ ุงูุฃุตููุฉ  
- ุงููุณุจุฉ ุงููุทุจูุฉ  
- ุงููุงุชุฌ ุงูููุงุฆู  

๐ ุนูุฏ ุงูุชุดุงู ุชุนุฏููุงุช ูู ุงูููุงููู ุฃู ุงูููุงุฆุญ:
- ุงุนุฑุถูุง ูู ุดูู ุฌุฏูู ููุณู ูุญุชูู ุนูู:
  | ุงูุจูุฏ | ุงููุต ุงููุฏูู | ุงููุต ุงูุฌุฏูุฏ | ุงูููุงุญุธุฉ ุฃู ููุน ุงูุชุนุฏูู |
- ุฅุฐุง ูู ููุฌุฏ ุงููุต ุงููุฏูู ูู ุงููููุ ุงูุชูู ุจุนุฑุถ ุงููุต ุงููุนุฏูู ูุน ุงูุฅุดุงุฑุฉ ุฅูู ุฑูู ุงููุงุฏุฉ.
- ุงุญุฑุต ุฃู ุชููู ุตูุงุบุฉ ุงูุฌุฏูู ููุธูุฉ ููุงุถุญุฉ ูููุณุชุฎุฏู.

๐๏ธ ููุงูู ุฎุงุตุฉ ูู ุงูุฃุณุฆูุฉ:
- ุฅุฐุง ุณุฃู ุงููุณุชุฎุฏู ุนู "ูุญุชูู ุงูููู" ุฃู "ูุง ูุญุชููู ุงูููู" ุฃู "ุงููุต ุงููุงูู"ุ
  ูุงูุชุฑุถ ุฃูู ูุฑูุฏ ุงุณุชุนุฑุงุถ ุงููุต ุงูููุฏู ุจุงููุงูู ุฃู ููุฎุตูุง ุชูุตููููุง ูู.
  ุนูุฏูุง:
  - ูุฏูู ุชูุฎูุตูุง ุดุงูููุง ููุถููู ุงููุต ุจูุถูุญุ ูุน ุฐูุฑ ููุน ุงููุณุชูุฏ.
  - ูุง ุชูู "ูู ูุชู ุฅุฑูุงู ููู"ุ ุญุชู ูู ูู ููุฐูุฑ ุงุณู ููู ุตุฑุงุญุฉ.
- ุฅุฐุง ุณุฃู ุนู "ููุฎุต ุงูููู"ุ ุฃุนุทู ููุฎุตูุง ูุฑูุฒูุง ูู ููุฑุงุช ุฃู ููุงุท.

๐ง ุฃุณููุจ ุงูุชูููุฑ:
- ุงููู ุงูููุฑุฉ ุฃูููุง ุซู ุญููู ุงูุชุทุจูู ุงูุนููู.
- ุงุฑุจุท ุจูู ุงูุจููุฏ ูุงูููุงุฏ ุฅุฐุง ูุงู ููุงู ุนูุงูุฉ ููุทููุฉ.
- ุนูุฏ ุงูุบููุถุ ุจููู ุฏุฑุฌุฉ ุงูุซูุฉ ("ูุคูุฏ"ุ "ูุญุชูู"ุ "ุงุณุชูุชุงุฌ").
- ูุง ุชุทูุจ ุฑูุน ุงูููู ุฅูุง ุฅุฐุง ูุงู {context} ูุงุฑุบูุง ูุนูุงู.

๐งฉ ูููุฐุฌ ุงูุชูุงุนู:
ุฅุฐุง ูุงู ููุงู ูุต ูู {context}:
> "๐ ููุน ุงููุณุชูุฏ: ..."  
ุซู ุงูุชุญููู ุงููุงูู.

ุฅุฐุง ูู ููุฌุฏ ูุต:
> "ูู ุฃุณุชูู ูุต ุงูููู. ุงูุฑุฌุงุก ุฑูุน ุงูููู ุฃู ูุตู ุงููุต ููุง."

ุงููุนุทูุงุช: {context}
ุณุคุงู ุงููุณุชุฎุฏู: {question}
ุงูุฅุฌุงุจุฉ:
"""



    # ูุนูู ูุงูุจ (Template) ููุฏุฑ ููุฑุฑ ูู ุงููุต ูุงูุณุคุงู
    prompt = PromptTemplate(template=PROMPT, input_variables=["context", "question"])

    # ูุฎุชุงุฑ ููุฏูู Groq (ุชูุฏุฑ ุชุบููุฑ ููุนู ุญุณุจ ุงุญุชูุงุฌู)
    model = ChatGroq(
        model="openai/gpt-oss-120b",  # ููุฏูู ุฐูู ูููุงุณุจ ููุฃุณุฆูุฉ ุงูุชุญููููุฉ
        temperature=0.2,       # ุฑูู ููุฎูุถ ูุนูู ุฅุฌุงุจุงุช ุฏูููุฉ ูุซุงุจุชุฉ
        groq_api_key=os.getenv("GROQ_API_KEY")  # ููุชุงุญ ุงูู API ูู ูุชุบูุฑ ุงูุจูุฆุฉ
    )

    # ูุญูู ุณูุณูุฉ ุณุคุงู ูุฌูุงุจ ุชุฑุจุท ุงูููุฏูู ุจุงูุจุฑููุจุช
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)

    # ูุฑุณู ุงูุณุคุงู ููููุฏูู ูุน ุงููุตูุต ุงููุฑุชุจุทุฉ ุจู
    response = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)

    # ูุฑุฌุน ุงููุต ุงูููุงุฆู ุงููุงุชุฌ ูู ุงูููุฏูู
    return response["output_text"]




def main():
  load_dotenv()
  st.set_page_config("ุงูุฑูุจูุช ุงูุฐูู", page_icon="๐ค")
  if "chat_history" not in st.session_state:
      st.session_state.chat_history = []
  st.title("๐ค ุงููุณุชุดุงุฑ ุงููุงูููู ุงูุฐูู")
  st.markdown("""
          <p style='text-align: center; color: #FFF7; font-family: Tajawal'>
            ุงุฑูุน ูููุงุชู ูุงุณุฃู ุฃู ุณุคุงู ุนููุง โ ุณูููู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุจุงูุฅุฌุงุจุฉ ุงุณุชูุงุฏูุง ุฅูู ูุญุชูู ุงูููู.
        </p>
""", True)
  st.write(css, unsafe_allow_html=True)
  st.markdown("""<div class='overlay'></div>""", True)
  PDFS = st.file_uploader("ุงุฑูุน ูููุงุชู ูู ููุง", type="pdf", accept_multiple_files=True)
  if PDFS:
    with st.spinner("โณ ุฌุงุฑู ุงููุนุงูุฌุฉ... "):
      #ุงุณุชุฎุฑุงุฌ ุงูููุงู ูู ุงููููุงุช ุงู PDF
      GET_TEXT = GET_TEXT_FROM_PDF(PDFS)

      # ุชูุณูู ุงููููุงุช ุนูู ุดูู ููุงุทุน
      SPLIT_TEXT_TO_CHUNK = SPLITTEXTTOCHUNK(GET_TEXT)
      for PDF in PDFS:
        filename = os.path.splitext(PDF.name)[0]
        STORE = CREATESTORE(SPLIT_TEXT_TO_CHUNK, filename)
      # ุงูุดุงุก ูุงุนุฏุฉ ุจูุงูุงุช

    user_question = st.chat_input("ุฃุณุงู ุณุคุงูู ููุง")
    if (user_question):

        answer = ASK_PDF_QUESTION(STORE, user_question)
        st.session_state.chat_history.append({
            "question": user_question ,
            "answer": answer
        })
        if st.session_state.chat_history:
            for chat in st.session_state.chat_history:
              # ๐น ูุญูู ุฃู ูููู ูุตู ุฅูู ุฑุงุจุท HTML ูุงุจู ููุถุบุท
              def make_links_clickable(text):
                # ูุญููู ุฃู ูููู (ุญุชู ุงููู ูู ุบูุฑ http) ุฅูู ุฑุงุจุท ูุงุจู ููุถุบุท
                url_pattern = r'((?:https?://)?(?:www\.)?[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[^\s<>"]*)?)'
                def repl(match):
                  url = match.group(0)
                  if not url.startswith("http"):
                    url = "https://" + url
                  return f'<a href="{url}" target="_blank" style="color:#4fc3f7; text-decoration:underline;">{match.group(0)}</a> ๐'
                return re.sub(url_pattern, repl, text)


              st.markdown(f"""
        <div style='background-color:rgb(255 255 255 / 4%); backdrop-filter: blur(10px); font-size:18px ;  color:#FFF; direction: rtl ; font-family:tajawal ;padding:10px; border-radius:10px; margin-top:10px;'>
            <span style="color: rgb(197 197 197); margin-bottom: 6px; display:inline-block; font-weight:bold"> ๐โโ๏ธ ุณุคุงูู</span><br>{chat["question"]}
                    </div>
  """, unsafe_allow_html=True)
              st.markdown(f"""
            <div style='background-color:#FFF1; font-size:18px ;color:#FFF; direction: rtl ;  font-family:tajawal ;padding:10px; border-radius:10px; margin-top:10px;'>
                        <span style='color:#009688; margin-bottom: 6px; display:inline-block; font-weight:bold'>๐ค ุฑุฏ ุงููุณุชุดุงุฑ ุงููุงูููู</span><br>{make_links_clickable(chat["answer"])}
                        </div>
  """, unsafe_allow_html=True)



if __name__ == "__main__":
  main()