import pydantic.v1
import streamlit as st
from langchain_groq import ChatGroq
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
import os
import pickle
import re;
from dotenv import load_dotenv
import fitz
from HTMLTEMPLATE import css
from langchain.schema import Document
# import pytesseract
# from PIL import Image




# pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # ุบููุฑ ุงููุณุงุฑ ุญุณุจ ููุงู ุงูุชุซุจูุช ุนูุฏู


def prepare_ocr_for_lm(text):
    """
    ุชูุธูู ุงููุต ุงููุงุชุฌ ูู OCR ูุจู ุฅุฑุณุงูู ูููููุฐุฌ
    """
    text = re.sub(r"\s{2,}", " ", text)
    text = re.sub(r"[^\u0600-\u06FF0-9A-Za-z\s.,()\-โโ/]", "", text)
    return text.strip()




def GET_TEXT_FROM_PDF(PDFS):
    """
    ๐ ุงุณุชุฎุฑุงุฌ ุงููุต ูู ูููุงุช PDF
    """
    full_text = ""

    for pdf in PDFS:
        pdf.seek(0)
        pdf_hash = hash(pdf.name)
        cache_file = f"cache_{pdf_hash}_smartocr.pkl"

        if os.path.exists(cache_file):
            with open(cache_file, "rb") as f:
                cached_text = pickle.load(f)
            st.info(f"๐ฆ ุชู ุชุญููู ุงููุต ูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ (Smart OCR Cache): {pdf.name}")
            full_text += cached_text
            continue

        pdf_text = ""

        doc = fitz.open(stream=pdf.read(), filetype="pdf")
        for i, page in enumerate(doc, start=1):
            text = page.get_text("text")
            if len(text.strip()) > 0:
                st.info(f"๐ ุงูุตูุญุฉ {i}: ุชุญุชูู ุนูู ูุต ูุงุจู ูููุณุฎ โ")
                pdf_text += text + "\n"
                continue

            st.info(f"โ ุงูุตูุญุฉ {i}: ูุง ุชุญุชูู ุนูู ูุตุ ุณูุชู ุงุณุชุฎุฏุงู Tesseract OCR...")

            # pix = page.get_pixmap(dpi=200)
            # img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

            # try:
            #     extracted = pytesseract.image_to_string(img, lang="ara+eng")
            # except Exception as e:
            #     st.error(f"โ ูุดู Tesseract OCR: {e}")
            #     extracted = ""

            # if extracted.strip():
            #     pdf_text += extracted + "\n"
            #     st.success(f"โ ุชู ุชุญููู ุงูุตูุญุฉ {i} ูุงุณุชุฎุฑุงุฌ ุงููุต ({len(extracted)} ุญุฑู).")
            # else:
            #     st.warning(f"โ ุงูุตูุญุฉ {i}: ูู ูุชููู OCR ูู ุงุณุชุฎุฑุงุฌ ุฃู ูุต.")

        pdf_text = re.sub(r"\s{2,}", " ", pdf_text).strip()

        with open(cache_file, "wb") as f:
            pickle.dump(pdf_text, f)

        full_text += pdf_text + "\n"

    st.success(f"๐ ุชู ุงุณุชุฎุฑุงุฌ ุงููุต ุจุงููุงูู ({len(full_text)} ุญุฑู).")

    cleaned_text = prepare_ocr_for_lm(full_text)
    return cleaned_text




def SPLITTEXTTOCHUNK(TEXT):
  SPLITTER = RecursiveCharacterTextSplitter(
    chunk_size=3500,
    chunk_overlap=500,
    separators=["\n", "ุงููุงุฏุฉ", ".", "ุ", " "]
  )
  CHUNK = SPLITTER.split_text(TEXT)
  return CHUNK


def CREATESTORE(TEXT, filename):
    """
    ๐ง ุฅูุดุงุก ุฃู ุชุญููู ูุงุนุฏุฉ ุจูุงูุงุช FAISS ุฎุงุตุฉ ุจูู ููู PDF.
    - ูุชู ุญูุธ ูู ููู ุฏุงุฎู ูุฌูุฏ faiss_index ุจุงุณู ุงูููู.
    """
    # ูุณุงุฑ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจุงูููู
    os.makedirs("faiss_index", exist_ok=True)
    PATH = os.path.join("faiss_index", f"{filename}")

    # ุงุฎุชูุงุฑ ูููุฐุฌ Embedding
    EMBEDDING = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    # ุงูุชุฃูุฏ ูู ูุฌูุฏ ููู ุงูููุฑุณ ูุจู ุงูุชุญููู
    if os.path.exists(f"{PATH}.index.faiss") and os.path.exists(f"{PATH}.index.faiss"):
        STORE = FAISS.load_local(PATH, EMBEDDING, index_name=filename ,allow_dangerous_deserialization=True)
        st.info(f"๐ฆ ุชู ุชุญููู ูุงุนุฏุฉ ุจูุงูุงุช {filename}")
    else:
        st.info(f"โ ุฌุงุฑู ุฅูุดุงุก ูุงุนุฏุฉ ุจูุงูุงุช ุฌุฏูุฏุฉ ููููู: {filename} ...")
        STORE = FAISS.from_texts(TEXT, embedding=EMBEDDING)
        STORE.save_local("faiss_index", index_name=filename)
        st.success(f"๐พ ุชู ุญูุธ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจุงูููู {filename} ุจูุฌุงุญ.")

    return STORE




def ASK_PDF_QUESTION(STORE, user_question):
    """
    ๐น ุงูุฏุงูุฉ ุฏู ุจุชุณุชูุจู ุณุคุงู ุงููุณุชุฎุฏูุ ูุชุณุชุฎุฏู ูุงุนุฏุฉ ุงูุจูุงูุงุช (FAISS)
    ุนูุดุงู ุชุฏูุฑ ุนูู ุฃูุณุจ ุฌุฒุก ูู ุงููุต ูุชุฌูุจ ุฅุฌุงุจุฉ ุฐููุฉ ูู ููุฏูู Groq.
    """

    context = st.session_state.get("context", "")

    # ูุจุญุซ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช ุนู ุงููุตูุต ุงูุฃูุฑุจ ููุณุคุงู
    docs = STORE.similarity_search(user_question, k=2)

    recent_context_text = ""
    if "chat_history" in st.session_state and len(st.session_state.chat_history) > 0:
        # ุฎุฐ ุขุฎุฑ 2 ูุญุงุฏุซุงุช (user+assistant)
        for chat in st.session_state.chat_history[-2:]:
            q = chat.get("question", "")
            a = chat.get("answer", "")
            recent_context_text += f"ูุญุงุฏุซุฉ ุณุงุจูุฉ โ ุณุคุงู ุงููุณุชุฎุฏู: {q}\nุฑุฏ ุงููุณุงุนุฏ: {a}\n\n"

    if recent_context_text:
        docs.append(Document(page_content=recent_context_text, metadata={"source": "recent_conversation"}))


    # ูุญุฏุฏ ุดูู ุงูุจุฑููุจุช (ุทุฑููุฉ ููู ุงูุณุคุงู)

    PROMPT = """
ุฃูุช ูุณุงุนุฏ ุฐูู ููุชุฎุตุต ูู ุชุญููู ุงููุตูุต ุงููุงููููุฉ ูุงููุงููุฉ ูุงูุฅุฏุงุฑูุฉ ูุงูุฃูุงุฏูููุฉ ูุงูุดุฎุตูุฉ.
ูููุชู ููู ุงููุต ูุชุญูููู ุจุนูู ูุงุณุชุฎูุงุต ุงููุนูููุงุช ูุงูุฅุฌุงุจุงุช ุงููุทููุจุฉ ุจุฏูุฉ ูุชูุธูู.

โ๏ธ ุงูููุงุนุฏ ุงูุนุงูุฉ:
- ุงููุชุบูุฑ {context} ูุญุชูู ุนูู ูุต ุงูููู ุงููุฑููุน.
- ุฅุฐุง ูุงู {context} ุบูุฑ ูุงุฑุบ โ ุงุนุชุจุฑ ุฃู ุงููุณุชุฎุฏู ุจุงููุนู ุฑูุน ููู.
- ูุง ุชูุฑุฑ ุนุจุงุฑุฉ "ูู ุฃุณุชูู ูุต ุงูููู" ุฅูุง ุฅุฐุง ูุงู {context} ูุงุฑุบูุง ุชูุงููุง.
- ูุง ุชุฐูุฑ ููุน ุงููุณุชูุฏ (ูุงููููุ ูุงูู...) ุฃูุซุฑ ูู ูุฑุฉ ูู ุงูุฌูุณุฉ ุงููุงุญุฏุฉ.
  โ ููุท ูู ุฃูู ุฅุฌุงุจุฉ ุชุญููููุฉ ุฃู ุฃูู ูุฑุฉ ููุฑูุน ูููุง ููู.
  ๐ ุจุนุฏ ุฐููุ ุฑููุฒ ูุจุงุดุฑุฉ ุนูู ุงูุฅุฌุงุจุฉ ุฃู ุงูุชุญููู ุงููุทููุจ ุฏูู ุฅุนุงุฏุฉ ุฐูุฑ ุงูููุน.
- ุฅุฐุง ุชุบููุฑ ุงูููู (ุฃู ุชู ุฑูุน ูุญุชูู ุฌุฏูุฏ ูุฎุชูู)ุ ุนุฏ ูุฐูุฑ ููุน ุงููุณุชูุฏ ูุฌุฏุฏูุง.

๐ฏ ุฃุณููุจ ุงูุชูุงุนู:
- ุงุณุชุฎุฏู ุฃุณููุจ ุงุญุชุฑุงูู ูุฏูุฏ ุจุงูุนุฑุจูุฉ ุงููุตุญู ุฃู ุงูุนุงููุฉ ุญุณุจ ุงููุณุชุฎุฏู.
- ุงุณุชุฎุฏู ุงูุฅูููุฌู ุงูุจุณูุทุฉ ููุชูุธูู (โ๏ธุ ๐ฐุ ๐ุ ๐งุ ๐ก).
- ูุง ุชูุฑูุฑ ููุฏูุงุช ุซุงุจุชุฉ ูู ูู ุฅุฌุงุจุฉ.

๐ ุงููุจุงุฏุฆ ุงูุนุงูุฉ:
1๏ธโฃ ุงุณุชูุชุฌ ููุน ุงููุณุชูุฏ ูู ุงููุต ุนูุฏ ุฃูู ูุฑุฉ ููุท.
2๏ธโฃ ูุง ุชุฎุชูู ูุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ูู ุงููุต.
3๏ธโฃ ูุฏูู ุงููุนูููุฉ ุจุฏูุฉุ ูุฅุฐุง ูุงูุช ุงุณุชูุชุงุฌูุง ูู ุฐูู ุตุฑุงุญุฉ.
4๏ธโฃ ูุง ุชุฐูุฑ ุฃู ุชุญูู ุฃุณูุงุก ูุคูููู ุฃู ุฌูุงุช ุฅูุง ุฅุฐุง ุทูุจ ุงููุณุชุฎุฏู ุฐูู.

๐ ุนูุฏ ุนุฑุถ ุฃูู ุชุญููู ููููู:
ุงุจุฏุฃ ุจู:
๐ **ููุน ุงููุณุชูุฏ:** (ูุงูููู / ูุงูู / ุฅุฏุงุฑู / ุฃูุงุฏููู / ุดุฎุตู)
ุซู ุงูุชุญููู ุญุณุจ ููุนู.

ุจุนุฏ ุฐููุ ูู ุงูุฑุฏูุฏ ุงููุงุญูุฉ:
โก๏ธ ูุง ุชูุฑุฑ ูุฐุง ุงูุณุทุฑ ูุฑุฉ ุฃุฎุฑู.
ุงุจุฏุฃ ูุจุงุดุฑุฉ ุจุงูุฅุฌุงุจุฉ ุนูู ุณุคุงู ุงููุณุชุฎุฏู.

๐ง ุฃูุซูุฉ ุณููู ุฐูู:
- ุฃูู ุณุคุงู: "ููุฎุต ุงูููู" โ ูุฑุฏ ุจููุน ุงููุณุชูุฏ ุซู ุงูููุฎุต.
- ุซุงูู ุณุคุงู: "ุฅูู ุงูุชุนุฏููุงุช ูู ุงููุงูููุ" โ ูุฑุฏ ูุจุงุดุฑุฉ ุจุงูุชุญููู ุจุฏูู ุฅุนุงุฏุฉ ุฐูุฑ ุงูููุน.
- ูู ุชู ุฑูุน ููู ุฌุฏูุฏ โ ูุฑุฌุน ูุฐูุฑ ููุน ุงููุณุชูุฏ ุฃููุงู ูู ุฌุฏูุฏ.

๐ ุงููุงุนุฏุฉ ุงูุฐูุจูุฉ:
- ุงูุงุณุชูุฑุงุฑูุฉ ุฃูู ูู ุงูุชูุฑุงุฑ.
- ุฐููุฑ ุงููุณุชุฎุฏู ููุท ุนูุฏ ุชุบูุฑ ุงููุญุชูู ุฃู ุงูุณูุงู.

ุงููุนุทูุงุช: {context}
ุณุคุงู ุงููุณุชุฎุฏู: {question}
ุงูุฅุฌุงุจุฉ:
"""




    # ูุนูู ูุงูุจ (Template) ููุฏุฑ ููุฑุฑ ูู ุงููุต ูุงูุณุคุงู
    prompt = PromptTemplate(template=PROMPT, input_variables=["context", "question"])



    # ูุฎุชุงุฑ ููุฏูู Groq (ุชูุฏุฑ ุชุบููุฑ ููุนู ุญุณุจ ุงุญุชูุงุฌู)
    model = ChatGroq(
        model="openai/gpt-oss-120b",  # ููุฏูู ุฐูู ูููุงุณุจ ููุฃุณุฆูุฉ ุงูุชุญููููุฉ
        temperature=0.2,       # ุฑูู ููุฎูุถ ูุนูู ุฅุฌุงุจุงุช ุฏูููุฉ ูุซุงุจุชุฉ
        groq_api_key=os.getenv("GROQ_API_KEY")  # ููุชุงุญ ุงูู API ูู ูุชุบูุฑ ุงูุจูุฆุฉ
    )

    # ูุญูู ุณูุณูุฉ ุณุคุงู ูุฌูุงุจ ุชุฑุจุท ุงูููุฏูู ุจุงูุจุฑููุจุช
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)

    context_text = "\n".join([doc.page_content for doc in docs])


    # ูุฑุณู ุงูุณุคุงู ููููุฏูู ูุน ุงููุตูุต ุงููุฑุชุจุทุฉ ุจู
    response = chain({"input_documents": docs, "question": user_question, "context": context_text if context_text.strip() else context}, return_only_outputs=True)

    # ูุฑุฌุน ุงููุต ุงูููุงุฆู ุงููุงุชุฌ ูู ุงูููุฏูู
    return response["output_text"]




def main():
  load_dotenv()
  st.set_page_config("ุงูุฑูุจูุช ุงูุฐูู", page_icon="๐ค")
  if "chat_history" not in st.session_state:
      st.session_state.chat_history = []
  st.title("๐ค ุงููุณุชุดุงุฑ ุงููุงูููู ุงูุฐูู")
  st.markdown("""
          <p style='text-align: center; color: #FFF7; font-family: Tajawal'>
            ุงุฑูุน ูููุงุชู ูุงุณุฃู ุฃู ุณุคุงู ุนููุง โ ุณูููู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุจุงูุฅุฌุงุจุฉ ุงุณุชูุงุฏูุง ุฅูู ูุญุชูู ุงูููู.
        </p>
""", True)
  st.write(css, unsafe_allow_html=True)
  st.markdown("""<div class='overlay'></div>""", True)
  PDFS = st.file_uploader("ุงุฑูุน ูููุงุชู ูู ููุง", type="pdf", accept_multiple_files=True)
  if PDFS:
    with st.spinner("โณ ุฌุงุฑู ุงููุนุงูุฌุฉ... "):
      #ุงุณุชุฎุฑุงุฌ ุงูููุงู ูู ุงููููุงุช ุงู PDF
      GET_TEXT = GET_TEXT_FROM_PDF(PDFS)

      st.session_state.context = GET_TEXT

      # ุชูุณูู ุงููููุงุช ุนูู ุดูู ููุงุทุน
      SPLIT_TEXT_TO_CHUNK = SPLITTEXTTOCHUNK(GET_TEXT)
      for PDF in PDFS:
        filename = os.path.splitext(PDF.name)[0]
        STORE = CREATESTORE(SPLIT_TEXT_TO_CHUNK, filename)
      # ุงูุดุงุก ูุงุนุฏุฉ ุจูุงูุงุช

    user_question = st.chat_input("ุฃุณุงู ุณุคุงูู ููุง")
    if (user_question):

        answer = ASK_PDF_QUESTION(STORE, user_question)
        st.session_state.chat_history.append({
            "question": user_question ,
            "answer": answer
        })
        if st.session_state.chat_history:
            for chat in st.session_state.chat_history:
              # ๐น ูุญูู ุฃู ูููู ูุตู ุฅูู ุฑุงุจุท HTML ูุงุจู ููุถุบุท
              def make_links_clickable(text):
                # ูุญููู ุฃู ูููู (ุญุชู ุงููู ูู ุบูุฑ http) ุฅูู ุฑุงุจุท ูุงุจู ููุถุบุท
                url_pattern = r'((?:https?://)?(?:www\.)?[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[^\s<>"]*)?)'
                def repl(match):
                  url = match.group(0)
                  if not url.startswith("http"):
                    url = "https://" + url
                  return f'<a href="{url}" target="_blank" style="color:#4fc3f7; text-decoration:underline;">{match.group(0)}</a> ๐'
                return re.sub(url_pattern, repl, text)


              st.markdown(f"""
        <div style='background-color:rgb(255 255 255 / 4%); backdrop-filter: blur(10px); font-size:18px ;  color:#FFF; direction: rtl ; font-family:tajawal ;padding:10px; border-radius:10px; margin-top:10px;'>
            <span style="color: rgb(197 197 197); margin-bottom: 6px; display:inline-block; font-weight:bold"> ๐โโ๏ธ ุณุคุงูู</span><br>{chat["question"]}
                    </div>
  """, unsafe_allow_html=True)
              st.markdown(f"""
            <div style='background-color:#FFF1; font-size:18px ;color:#FFF; direction: rtl ;  font-family:tajawal ;padding:10px; border-radius:10px; margin-top:10px;'>
                        <span style='color:#009688; margin-bottom: 6px; display:inline-block; font-weight:bold'>๐ค ุฑุฏ ุงููุณุชุดุงุฑ ุงููุงูููู</span><br>{make_links_clickable(chat["answer"])}
                        </div>
  """, unsafe_allow_html=True)



if __name__ == "__main__":
  main()